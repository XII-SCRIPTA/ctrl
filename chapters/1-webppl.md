---
layout: chapter
title: "Probabilistic programming in WebPPL"
description: "A tutorial introduction to probabilistic programming in WebPPL."
---


> We begin with a quick overview of probabilistic programming. If you are new to probabilistic programming, you might want to read an informal introduction (e.g. [here](http://www.pl-enthusiast.net/2014/09/08/probabilistic-programming/) or [here](https://moalquraishi.wordpress.com/2015/03/29/the-state-of-probabilistic-programming/)) or a more technical [survey](https://scholar.google.com/scholar?cluster=16211748064980449900&hl=en&as_sdt=0,5). 

## Introduction

This chapter introduces **WebPPL** ("web people"), a **probabilistic programming language** (PPL). All models and experiments in this thesis are implementated in WebPPL, so it is important to understand how the language works at a basic level. Broadly construed, probabilistic programming languages are tools for statistical modelling and analysis. In their simplest form they are a deterministic programming language augmented to include conditioning and random sampling operators. In the case of WebPPL, the deterministic language is a purely functional subset of Javscript.

## The Benefits of WebPPL

WebPPL implementations provide a number of benefits over other machine learning frameworks. Firstly, WebPPL programs can be written, and executed within a web browser, making code sharing and experimentation straightforward. Secondly, there are the ability to rapidly prototype and test models is made possible since a number of commonly used sampling and approximation algorithms are standard methods built into the language. This greatly simplifies the process of statistical analysis, verification, and evaluation components of probabilistic models. To clarify how this process might work, and to motivate the power of PPLs, we now consider a simple probailistic model given below.

To clarify how this process works in a highly simplified experimental setting (and the motivate the power of PPLs) we now consider a simple probailistic model given below.

> **Example: Coin flipping** Julia flips a coin ten times, and observes the following outcomes `[0, 1, 0, 1, 0, 0, 0, 0, 1, 1]`, She writes down `0` when the coin lands tails, and `1` when it lands heads. In particular, "She is interested in the probability that the coin lands heads. To analyze this, she first builds a model: suppose she assumes the coin flips are independent and land heads with the same probability. Second, she reasons about the phenomenon: she infers the model's hidden structure given data. Finally, she criticizes the model: she analyzes whether her model captures the real-world phenomenon of coin flips. If it doesn't, then she may revise the model and repeat.

This process involves three steps: 1. build a probabilistic model of the phenomena. 2. reason about the phenomena given model and data. 3. criticize the model, revise and repeat. The purpose of constructing such models is to query them, observing their output under various conditions. The resulting observations generated by the model can be used to empirically verify the natural phenomena that the analyist wishes to model. Suppose Julia had the prior assumption that if the coin is fair, we should expect the probability of heads to be roughly 50%. If the coin is unfair, we should expect a disproportionate number of `0`'s  or `1`'s. Again, the power of PPLs allows us to straightforwardly test this hypothesis by modifying the program that we wrote above. This will allow us to model the behavior of different types of coin weights.

To test this hypothesis, we can write run a simulation of this experiment using a very simple WebPPL program.

~~~
repeat(10,flip)
~~~

We can then use the built in visualization library, accessed via the keyword `viz` to construct a histogram over program outputs.
~~~
viz(repeat(10,flip))
~~~

> Run this program a few times. You will get back a different sample on each execution.

What if we want to invoke this sampling process multiple times? We would like to construct a stochastic function that adds three random numbers each time it is called. We can use function to construct such complex stochastic functions from the primitive ones.

~~~
var sumFlips = function() { return flip() + flip() + flip() }
viz(repeat(100, sumFlips))
~~~

## Bayesian inference by conditioning

The most important use of inference methods is for Bayesian inference. Here, our task is to *infer* the value of some unknown parameter by observing data that depends on the parameter. For example, if flipping three separate coins produce exactly two Heads, what is the probability that the first coin landed Heads? To solve this in WebPPL, we can use `Infer` to enumerate all values for the random variables `a`, `b` and `c`. We use `condition` to constrain the sum of the variables. The result is a distribution representing the posterior distribution on the first variable `a` having value `true` (i.e. "Heads").

~~~~
var twoHeads = Infer({
  model() {
    var a = flip(0.5);
    var b = flip(0.5);
    var c = flip(0.5);
    condition(a + b + c === 2);
    return a;
  }
});

print('Probability of first coin being Heads (given exactly two Heads) : ');
print(Math.exp(twoHeads.score(true)));

var moreThanTwoHeads = Infer({
  model() {
    var a = flip(0.5);
    var b = flip(0.5);
    var c = flip(0.5);
    condition(a + b + c >= 2);
    return a;
  }
});

print('\Probability of first coin being Heads (given at least two Heads): ');
print(Math.exp(moreThanTwoHeads.score(true)));
~~~~

### Codeboxes and Plotting

The codeboxes allow you to modify our examples and to write your own WebPPL code. Code is not shared between boxes. You can use the special function `viz` to plot distributions:

~~~~
var appleOrangeDist = Infer({
  model() {
    return flip(0.9) ? 'apple' : 'orange';
  }
});

viz(appleOrangeDist);
~~~~


