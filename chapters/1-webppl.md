---
layout: chapter
title: "Probabilistic programming in WebPPL"
description: "A tutorial introduction to probabilistic programming in WebPPL."
---


> We begin with a quick overview of probabilistic programming. If you are new to probabilistic programming, you might want to read an informal introduction (e.g. [here](http://www.pl-enthusiast.net/2014/09/08/probabilistic-programming/) or [here](https://moalquraishi.wordpress.com/2015/03/29/the-state-of-probabilistic-programming/)) or a more technical [survey](https://scholar.google.com/scholar?cluster=16211748064980449900&hl=en&as_sdt=0,5). For a practical introduction to both probabilistic programming and Bayesian modeling, we highly recommend [ProbMods](http://probmods.org), which also uses the WebPPL language. 

## Introduction

This chapter introduces **WebPPL** ("web people"), a **probabilistic programming language** (PPL). All models and experiments in this thesis are implementated in WebPPL, so it is important to understand how the language works at a basic level. Broadly construed, probabilistic programming languages are tools for statistical modelling and analysis refp:Ghahramani:2015, refp:Tolpin-etal:2018. In their simplest form they are a deterministic programming language augmented to include conditioning and random sampling operators refp: Goodman:2013, refp:goodman-stuhlmuller-dippl:2014, refp:Rainforth:2017. In the case of WebPPL, the deterministic language is a purely functional subset of Javscript.

## Building Probabilistic Models

"Run this program a few times. You will get back a different sample on each execution. Also, notice the parentheses after `flip`. These are meaningful; they tell WebPPL that you are calling the flip function—resulting in a sample. Without parentheses flip is a function object—a representation of the simulator itself, which can be used to get samples."

To clarify how this process works in a highly simplified experimental setting (and the motivate the power of PPLs) we now consider a simple probailistic model given below.

> **Example: Coin flipping** Julia flips a coin ten times, and observes the following outcomes `[0, 1, 0, 1, 0, 0, 0, 0, 1, 1]`, She writes down `0` when the coin lands tails, and `1` when it lands heads. In particular, "She is interested in the probability that the coin lands heads. To analyze this, she first builds a model: suppose she assumes the coin flips are independent and land heads with the same probability. Second, she reasons about the phenomenon: she infers the model's hidden structure given data. Finally, she criticizes the model: she analyzes whether her model captures the real-world phenomenon of coin flips. If it doesn't, then she may revise the model and repeat.

To test this hypothesis, we can write run a simulation of this experiment using a very simple WebPPL program.

~~~
repeat(10,flip)
~~~

We can then use the built in visualization library, accessed via the keyword `viz` to construct a histogram over program outputs.
~~~
viz(repeat(10,flip))
~~~

This process involves three steps: 1. build a probabilistic model of the phenomena. 2. reason about the phenomena given model and data. 3. criticize the model, revise and repeat. The purpose of constructing such models is to query them, observing their output under various conditions. The resulting observations generated by the model can be used to empirically verify the natural phenomena that the analyist wishes to model. Suppose Julia had the prior assumption that if the coin is fair, we should expect the probability of heads to be roughly 50%. If the coin is unfair, we should expect a disproportionate number of `0`'s  or `1`'s. Again, the power of PPLs allows us to straightforwardly test this hypothesis by modifying the program that we wrote above. This will allow us to model the behavior of different types of coin weights.

Try clicking "Run" repeatedly to get different i.i.d. random samples:

~~~~
print('Fair coins (Bernoulli distribution):');
print([flip(0.5), flip(0.5), flip(0.5)]);

print('Biased coins (Bernoulli distribution):');
print([flip(0.9), flip(0.9), flip(0.9)]);

var coinWithSide = function(){
  return categorical([.45, .45, .1], ['heads', 'tails', 'side']);
};

print('Coins that can land on their edge:')
print(repeat(5, coinWithSide)); // draw 5 i.i.d samples
~~~~

"What if we want to invoke this sampling process multiple times? We would like to construct a stochastic function that adds three random numbers each time it is called. We can use function to construct such complex stochastic functions from the primitive ones."

~~~
var sumFlips = function() { return flip() + flip() + flip() }
viz(repeat(100, sumFlips))
~~~



## "Hypothetical Reasoning with `Infer`"

THIS IS USED FOR MODEL EXPLANATION

Suppose Julia encounters an odd set of observations, and she wants to infer the **latent** probability that the coin will end up heads.

> **Example: Trick Coin flipping** Julia flips another coin ten times, and observes the following outcomes `[0, 1, 0, 0, 0, 0, 0, 0, 0, 1]`. She wants to infer the latent probability of that the coin will land heads, which in the case above, is 3/10. On face value, (a **prior** assumption) it seems unlikely that the coin that generated the above outcomes is fair. To test this in a rigorous way, she can run the experiment many times over to get an **estimate** or **expected value** of the coin's tendency to land heads.

"Suppose that we know some fixed fact, and we wish to consider hypotheses about how a generative model could have given rise to that fact. In the last chapter we met the Infer operator for constructing the marginal distribution on return values of a function; with the help of the condition operator Infer is also able to describe marginal distributions under some assumption or condition."

~~~
var model = function() {
  var A = flip()
  var B = flip()
  var C = flip()
  var D = A + B + C
  return {'D': D}
}
var dist = Infer({}, model)
viz(dist)
~~~

"The process described in model samples three numbers and adds them (recall JavaScript converts booleans to 00 or 11 when they enter arithmetic). The value of the final expression here is 0, 1, 2 or 3. A priori, each of the variables A, B, C has .5 probability of being 1 or 0. However, suppose that we know that the sum D is equal to 3. How does this change the space of possible values that variable A could have taken? A (and B and C) must be equal to 1 for this result to happen. We can see this in the following WebPPL inference, where we use condition to express the desired assumption:"

~~~
var model = function () {
  var A = flip()
  var B = flip()
  var C = flip()
  var D = A + B + C
  condition(D == 3)
  return {'A': A}
};
var dist = Infer({}, model)
viz(dist)
~~~

"The output of `Infer` describes appropriate beliefs about the likely value of `A`, conditioned on `D` being equal to `3`.

Now suppose that we condition on `D` being greater than or equal to 2. Then `A` need not be 1, but it is more likely than not to be. (Why?) The corresponding plot shows the appropriate distribution of beliefs for `A` conditioned on this new fact:"

~~~
var model = function () {
  var A = flip()
  var B = flip()
  var C = flip()
  var D = A + B + C
  //add the desired assumption:
  condition(D >= 2)
  return {'A': A}
};
var dist = Infer({}, model)
viz(dist)
~~~


## "Constructing marginal distributions: `Infer`"

"Above we described how complex sampling processes can be built as complex functions, and how these sampling processes implicitly specify a distribution on return values (which we examined by sampling many times and building a histogram). This distribution on return values is called the marginal distribution, and the WebPPL Infer operator gives us a way to make this implicit distribution into an explicit distribution object:"

~~~
//a complex function, that specifies a complex sampling process:
var foo = function(){gaussian(0,1)*gaussian(0,1)}

//make the marginal distributions on return values explicit:
var d = Infer({method: 'forward', samples: 10000}, foo)

//now we can use d as we would any other distribution:
print( sample(d) )
viz(d)
~~~




## The Benefits of WebPPL

WebPPL implementations provide a number of benefits over other machine learning frameworks. Firstly, WebPPL programs can be written, and executed within a web browser, making code sharing and experimentation straightforward. Secondly, there are the ability to rapidly prototype and test models is made possible since a number of commonly used sampling and approximation algorithms are standard methods built into the language. This greatly simplifies the process of statistical analysis, verification, and evaluation components of probabilistic models. For instance, WebPPL includes a number of built in primitive distributions, like Bernoulli, Gaussian, and Dirichlet, which can be used to generate samples and perform density mass calculations. To clarify how this process might work, and to motivate the power of PPLs, we now consider a simple probailistic model given below (adapted from [edward api])










## WebPPL: a purely functional subset of Javascript


There are no `for` or `while` loops. Instead, use higher-order functions like WebPPL's built-in `map`, `filter` and `zip`:

~~~~
var xs = [1, 2, 3];

// Don't do this:

// for (var i = 0; i < xs.length; i++){
//   print(xs[i]);
// }


// Instead of for-loop, use `map`:
map(print, xs);

"Done!"
~~~~



### Bayesian inference by conditioning

The most important use of inference methods is for Bayesian inference. Here, our task is to *infer* the value of some unknown parameter by observing data that depends on the parameter. For example, if flipping three separate coins produce exactly two Heads, what is the probability that the first coin landed Heads? To solve this in WebPPL, we can use `Infer` to enumerate all values for the random variables `a`, `b` and `c`. We use `condition` to constrain the sum of the variables. The result is a distribution representing the posterior distribution on the first variable `a` having value `true` (i.e. "Heads").

~~~~
var twoHeads = Infer({
  model() {
    var a = flip(0.5);
    var b = flip(0.5);
    var c = flip(0.5);
    condition(a + b + c === 2);
    return a;
  }
});

print('Probability of first coin being Heads (given exactly two Heads) : ');
print(Math.exp(twoHeads.score(true)));

var moreThanTwoHeads = Infer({
  model() {
    var a = flip(0.5);
    var b = flip(0.5);
    var c = flip(0.5);
    condition(a + b + c >= 2);
    return a;
  }
});

print('\Probability of first coin being Heads (given at least two Heads): ');
print(Math.exp(moreThanTwoHeads.score(true)));
~~~~



### Codeboxes and Plotting

The codeboxes allow you to modify our examples and to write your own WebPPL code. Code is not shared between boxes. You can use the special function `viz` to plot distributions:

~~~~
var appleOrangeDist = Infer({
  model() {
    return flip(0.9) ? 'apple' : 'orange';
  }
});

viz(appleOrangeDist);
~~~~


### Next
